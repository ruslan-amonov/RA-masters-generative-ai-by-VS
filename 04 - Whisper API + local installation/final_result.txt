 Good evening guys. Good evening. Hello. You choose Mr Curtis all? If they are anyone who has an eyes to maybe 30 minutes ago. It seems to be a bit collective. By the way, do we have strict deadlines for homework guys? No. Isn't it that strict? No, no, no, it's not strict. I did. It works by the... But I would suggest to find at least like 10 to 15 minutes every day how I am doing, because when it will come at the end, it will be like everything, you know. So at least I'm just trying to find 10, 15, 20 minutes doing, doing like by small, small, small parts. Then... Homework from the rest of the day, right? Discourse or from others? Any, I think. Other courses, I think, they didn't give you homework yet, yeah? Or they gave it? Data, data processing they gave, it gave, he gave yesterday, I think, this file format. I missed the class yesterday. I'll watch it today. Hello, Vitaly. Hello, Vitaly. How are you doing? Vitaly, I have a question about the future. If you see, so two days ago, Google introduced quantum computers. Yeah. Our second lesson, you said that there is a problem with the models because they have a resource problem. What will be in the future about it? Because if Google, if we have, they have a quantum computers, then they have a huge resource. And how they will change? Yeah. So that's really good, a interesting question. So I didn't know how they work, like only very general concept of these QBs. I still, I've been studying quantum physics in university, like at least one year, probably two years, I can't remember. A lot of electronics works on quantum effects. So for example, some stuff allows the electrons to jump the barrier. They actually cannot jump, so they tunneling through that. And no one knows how it works, but it works. And we use it in TVs and smartphones. And that's it. Some kind of magic. So it's still, I think there should be some kind of scientific explanation, but I didn't know how these quantum computers work. I don't understand the concept of something having the value of zero and one at the same time, because, like, okay, you can imagine that, but how about the storage? Right? So if I talk about like HDD or SSD, any storage, so you need to write it. And you need to, like, you should be able to read it in the future. Basically, the most fear, like two fears I remember, like from the community regarding quantum computers is, okay, if they are so powerful and can make like much more, like, I don't know, millions times computations per second. So what about the encryption, right? So RSA or any other encryption algorithm. Basically, encryption works, like modern encryption works on the very basic example, like there are, how is called like one way function in maths. I don't know how it's in English. So the way, like, it's very easy to calculate this, for example, it's very easy to multiply two prime integers and get the result, but it's super hard to understand, like, what are the integers you multiply it if you receive, like, this result. And a lot of encryption is based on that and actually Bitcoin is based on that. So Bitcoin has several, like Bitcoin has a lot of protocols inside, like technologies and most of them, like top tier encryption we have right now. So if Google can beat that, so they can possibly, like, hijack all the new Bitcoins. Maybe they can rewrite the whole Bitcoin, like the whole blockchain, like in a day and, I don't know, like steal all the Bitcoins from us. So a lot of like military applications, like all this encryption and networking. So, I don't know, it's technically possible that such technology can appear, but taking an account how much time and money. And resources and smart people it takes to build such thing. So maybe the first country which builds it, then, you know, like, who creates some kind of like, Snow Crash, like from cyberpunk books, I don't know. So we'll see. At least, like, I know, like, from what I understand, like IT history and technology history, like, all the new, like, top tier technologies, they have, like, two applications, like the two industries, basically driving them fast in making them broadly available is porn industry and military. So this is like two things, usually picking up the latest stage and trying to get, like, money of debt. And maybe one of them will pick it up. I don't know, maybe military first. So, again, they see the new applications of quantum stuff. Yeah, but from personal perspective, like being a human, like, you know, like IT guys. So yeah, that's interesting. But unless I cannot benefit from that, I cannot, like, earn money doing that. So it's inevitable for me, like the sun, the sun just can stop. I don't know, like, okay. But the example because the earth rotates across the sun, so the earth can just stop rotating and it will be infinite night for me. So I can't do anything about that. I can't do anything about rain. So what I can is just to buy the umbrella. I don't know, pray during stops sometimes. And in case I have the problem or some, some thing I cannot influence. I think it's good not to worry about it. Because if you worry about something you cannot change, that's bad. So you spent a lot of resources and the result will be nothing like for sure. So my plan is just read the news. I don't know, follow this agenda, but don't worry about it. Yeah, let's, let's get down to business. So let me try to open my, our schedule. I have to reboot my PC today so I lost this ex was precious. Okay, this one. Okay, so the good news, good news. We are on track, right. And here, here we are, meeting number five. So today we're going to discuss whisper API and local installation. The bad news. I took a look at some of the Chrome works, like two or three, maybe five, but not like very, very high level. I like the blog posts you guys made. So I like the pictures and I think I will share some of my favorite pictures next time. So I assume I will need this weekend to check them and probably to add in Alex at some marks or something like that. So just give me some time, but I haven't found any like critical problems there. So just remember to, to share the workflow, how you, how you did that, like for the first task. So I need to see like what prompts you've been using and don't forget about the pictures. So the pictures are necessary for our blog post. So I think we're going to focus on, on that probably either on this lecture or this one. We're not have time to review the homework. So today we're going to discuss the whisper API and this is very, let me show the slides. So here, here are the slides for today. So what is the whisper and why we're discussing that? So mostly when people discuss the generative for AI or AI in general, so a focused on like LLMs, I know like copilot's functions and stuff like that. So everything that works with text, right? Because it has more application to the business when you work with text. But still, usually what is being forgotten is images and audio processing. And regarding the images, I can't say this like easier to monetize this. So not too much business applications. So I know a lot of, I know a lot of cases where clients came to us like to the palm and asks to create something. Like, I don't know, maybe a rate of over a hundred of cases. And only, only several of them were related to image generation, but some of them, like more, more of these cases are related to audio. And one of the usual like request is, okay, guys, we have 5000 hours of recordings of, I don't know, meetings or customers calling us for the support. So we need to understand this data, right? So imagine you have a first line support like people calling, like real users calling describing the issue and the operators are just sharing. So what should you do to solve it? And you need to control it somehow, right? So you need to see what are the most popular questions or what are the answers. So is the client is happy or not, right? And for this case, you need to voice processing and processing the voice is actually very old technology. I don't remember how old it is, but like two things here usually working is text-to-speech and speech-to-text. And if we check, I didn't know like, let's check Azure text-to-speech. Okay, it's a yeah, it's speech now, all right, I got it. Everything is a yeah now. Let me check the pricing actually. TTS pricing, it's super cheap. It's super cheap. Okay, I think here. So this is, okay, free tier, all right. Basically, all right, delete this one. So speech-to-text. So in order to transcript something like if you have a phone call, real-time transcription, $1 per hour, that's too much. Okay, batch transcription. So if I have a lot of audio calls, I didn't know like one hungry, I can post them as a batch and this will result me in 80 cents per hour. So not too much actually. Yeah, as usually, like Microsoft has like very complicated, very complicated stuff for billing, so it's hard to understand how much you will spend. Yeah, and here is like the text-to-speech. So basically, two directions like from speech, you can extract text and from text, you can generate speech, right? And this is like the standard voice. I think this standard voice become neural like during like last year because previously they had different pricing. Like the standard voice is like this robotized voice you usually hear when you call a bank or something. And neural voice is like more pleasant, more realistically, more naturally told. So right now I don't think they should have some kind of like cheap voice like this old cheap voice. Probably it's inside the Azure EA studio, but the truth is it's not expensive, right? So one billion characters, it's like it's a lot. $15 for business, it's like well, not too much I think. So and in case we have 1000 of like phone calls transcripted, so it will result as only like less than $200. So that's acceptable for that. And they definitely use machine learning there, right? So because there's no way programmatically like algorithmically to transcribe this page. But still the interesting thing here is it's also possible to use machine learning techniques and all this concept of token we've been discussing previously, right? In working with speech in understanding the speech. And the project, the project I would like to show you this whisper. So let me find it should be here in the notes I put. Yeah, this one. It was introduced very similar once the child GPT arrived, right? So, I'm just trying to recognize what was the time when charge you can see released. Okay. Yeah, chat GPT. At least. Okay, November 30. All right, 2022. Yeah, so they introduced this per like couple of months before charge PT. And to know what happens like do you remember this movie? The Sutton's floor. This one. So do you know this movie? It's kind of popular from scientific perspective, but they've got the problem right so this is the cyber like not so cyberpunk but probably like I think science fiction, but take a look at the release years 1999. The problem is that in 1999, this movie gets out matrix. And that's why like, everyone known about the matrix. It was like very successful movie. And this is the reason much more less people know that this movie for even exists. If they release it like a year previous before matrix or much more success or the same as a whisper right so charge PT release November 14 and whisper actually the same company open the eye but it was not so hyped. In September, as it become in November right. So what they introduced is they actually been working like at the same time looks like they've been working at the whisper and parallel like working with charge of PC right so they reused the same, like not the same but similar architecture like encoders decoders and tokens to predict the tokens but not from text, but from audio as well. So they just take a look at the idea right and in the paper so they share like this one. So they took 680,000 hours of multi lingual audio. So they sliced it in 30 seconds time like steps, not steps but chunks and they, the train the model, the train the model against it using the same approach. They useful for the GPT. So pretty the same like slice the data into some some samples. Try to predict the tokens like compare with the result learn train the model and so on. So, and this results in a very interesting thing. Very high quality and I don't know if there are any model right now which can beat the quality of whisper because once it was released, it was like very high quality they released another model. A couple of months ago, I think it's called whisper but the architecture seems to stay to stay the same. So, and one more interesting thing here is, it's completely open sourced. So you can download it and it doesn't require a lot of VRAM to run so you can run it on the GPU. You can run it a bit slower on the CPU. And as far as it's like not language model, it will perform like with acceptable, acceptable speed. I've been running it at the CPU once and it works normally so not so slow as the LLM. So we are not so interested in technical details and I don't understand them actually just to explain to you. But what we're interested in is actually we interested in the result right so how it works and how I can use it. So how to use it so you just install it as a Python package but you need by touch. And this is like why I mentioned the pytor's previously so in order to install it locally. So you must have the pytor's and this is the command. It should be pytor's. Download probably now. Come on, where's download. Get started. Oh here install. Okay, like if you would like to try this pair right or in future if you would like to work with pytor's so there's like two options here. So it doesn't matter which version you will be installing you're going to see the same table for every time like it's couple of years the same. So if you're installing it like in straightforward like recommended way. Like usually it's windows, Python, package, or pip, Python, and here is like CUDA or CPU and depending on what you will select the URL will be different right. So CUDA is the accelerated computing framework created by NVIDIA as far as I remember by NVIDIA to run the computations on the GPU. And if you download and install this pytor's you will be running everything against the GPU. But if you do not have GPU right so you can install it without like just the CPU and it will be like the same pytor's it would work the same but much more slower right. But it will still work. So you need to you need to prior to install the pytor's so first you need to Python. Next you need to select the CUDA version so usually the later the better you just copy this and install. And actually I did that today because my pytor's. Well, not pytor's but whisper fade felt around today so I've been preparing to the demo and I also have like the some files. Audio files so and the problem is so why facing them and you may face the same. So let me let me show you so the problem with Python is that Somehow like for for for modern applications I use NumPy version two so NumPy is a framework is a library to work in like with numbers in Python and it's required by pytor's. But this this pytor's and it's like implementation in whisper doesn't work with NumPy 2.0.2 so it requires like old version and that's this kind of problem you may face as well so what I did is actually I installed Like virtual environments so I created the virtual Python environment and downloaded once again pytor's whisper and all the stuff so at least now it works right so that's that's the problem you may face with working with any software in Python. At least I think it should be some kind of work around but taking a look at how many people using the virtual environments. I think this is this is the acceptable work around so getting back here so what you need is just to install the library and that's it you can install it from source but I prefer like this lazy option and it also requires ffm pack so ffm pack. This this is the most popular library to working with audio and video so. And actually this may be useful for you in future so I know like working in development or. Just processing media like audio and video requires you to convert files from one format to another split them I don't know change the bitrate and anything so everything can be done with ffm pack but ffm pack. Is a command line interface like it's a program which allows you to do everything but without the user interface without the UI but you can you can work with it programmatically so for example this is how you can convert from and before to. Maybe like the old format right and this is like the the greatest software and the fastest solution right now I think so a lot of. Audio and video converting software use ffm pack under the hood so it will be necessary as well if you would like to start with whisper because whisper will need to transform the. Data from one from or to another so for example in my examples I'm gonna use MP3. But in some cases it may be one format or something like that so this turn you to try to transform from one format to another and here are the models and languages so as far as it's. Machine learning model right so you can pick up any model and getting back here so remember we've been discussing this like model card. Quantization distillation and so on so different models right so this is like 13 billion model this is 70 billion model the same as whisper. So this model has 39 million parameters it's tiny and it requires only one gigabyte of video RAM like VRAM and it's very fast rate so assuming the large model as like 1.5 billion parameters to occupy 10 gigabytes of VRAM. And the speed is one like one X right so this one is 10 times faster and turbo is eight times faster and that's it they have large V2 and large V3. And what's more interesting here is the quality so the quality here is measured in WER like work error rate. So it just evaluating how much how many words are like. Mis predicted like like how many words are wrong and you can you can see the languages here so depending on the data set and probably the language structure. This really differs right so I'm not surprised the Spanish is the has like less errors than any other languages because Spanish at least from my perspective I know like Apollo 4 and. It's it looks like very easy language right. Like straightforward and kind of easy. Yeah and some of the languages probably lacking the presence in the Internet will result in like half of the words misspelled like Belarusian. I don't know I think I need to I know Belarusian language and I think I need to find some podcast in Belarusian and try to feed it into the whisper and check like how it works. That's that's someone strange because 42 out of 100 comparing to six in Ukrainian so Belarusian and Ukrainian are super similar super similar so I can understand like 90% of Ukrainian and Ukrainian understand 90% of Belarusian probably is the problem in the data set right. Okay, but in case like in most cases we're gonna work with English right. So working with these languages are super cool. And the command line users just pretty easy. So let's run the let's run the example and for the example I have the. I took the matrix movie and you know the like there is a scene. And just me to stock in with more foods. Let me just matrix engines meet. Is his delivering the speech about like the humans. I'm not sure you will be able to. To hear it because I'm using teams in in browser so it's not desktop application so I can't share my audio, but here is like the agents meet is interrogating more first and he's delivering this pitch like I got the idea so human beings are a virus and we like the agents are the cure. So I will drop you the link or you can just Google it and and reconsider this is I picked this sample because like high quality speech and know any like music in the ground and something like that. So let me try to run this and check how it will work. So default. Whisper. We just need to call whisper and pass the parameter as the data file. So right now is detecting the language isn't first 30 seconds and the language is detected to English. So you can do that actually manually you can specify the language and here is the output rate. So this is like the Smith is. Discussing it this is very precise. This is very precise. I think it's like. Probably. Here we have the problems. Yeah, I think it's messing it's messing something still, but I don't know what model I'm use so let me call the. The usage. Okay, so a lot of things here. So we are interested in the model. So we can specify as a model from here right. So these are the available models. I don't know which one is the default maybe small or base. So I have a lot of VRAM so I can actually launch launch model. So let's try with probably another model and you can also specify the language explicitly. I don't think it will help us because it automatically recognize that this is English. One more thing actually very interesting thing. You can do with this pro. So here should be a parameter which is called prompt. You find it. Tres. Okay, model device output verbose. They should be parameter which is called something like a prompt. No. Yeah, this one initial prompt. So the trick is so by default it's none but let's get back to the idea so this pair. So this pair works with tokens right and everything like in this architecture you can you can pass the system message so you can pass the system message and whisper and I've been working once on English transcription task. So we decided to make to transcribe the English assessments in our company so there is like an interview asking questions and the person replying and one of the idea was to calculate the. I don't know how it's called like the words like when the person is thinking like parasite words. So it's not possible to do with just plain whisper. Hardly possible but if you explicitly provide them in a prompt it will recognize them and also a lot of a lot of times in this dialogue. There were mentioned a company name upon and there is no such word in in whisper data set so that's why it was looking for a similar but if you provide it in initial prompt, and it will like see the same token combination. It will just do not search for any other alternative so this is very powerful thing like on very underrated. Okay, so let's get back here and let's try the model. Let me try different models and we're going to see the performance like this speed. Let's try with tiny the smallest model and let's check how fast it will work. Pretty fast. And let me try. Turbo. Okay, pretty fast as well. I don't see a big difference in speed here, right? At least at this sample. The sample is on the one minute. Okay, yeah, the final token was took the time and let me try large. So the large should give us the highest quality. Okay, yeah, you see the difference. So large and turbo versus the tiny. So the tiny, you can just understand what's what's going on in this discussion. If you don't care about particular worlds, if you just would like, I don't know, to summarize it in future and make some decision like, I don't know, sentiment analysis or something so maybe tiny is okay, but still, that's a lot of questions as far as I see. But here the turbo and large so they work very effectively. All right. So, and once I run every model so it's getting downloaded. So, I don't think I started small. So probably it will be downloading right now. Now it's already downloaded. I think. Yeah, so once I selected large first time. It started downloading all of this model like one and a half gigabytes of model locally. So if you have a lot of files, I didn't know, million of hours. So what you just need, you just need the PC. So you can use it in cloud, right? So let's check the pricing. Open the icon API pricing. So you can do that with API. Let me scroll to the whisper by tuning real time systems. Okay images, audio models. Okay. So what we're going to pay, we're going to pay 0.16 cents per minute. So let's let me calculate one hour. So 0.06 cents per minute. So it means one hour will cost us 36 cents. Yeah, so it means like 100 hours of transcription will result in $36. So there should be some point where it's cheaper to purchase hardware and run it on your hardware and save the hardware for yourself instead of sending to the to the open the right. Yeah, so in some cases it's more effective to do it on your own machine. I didn't know like spend the night. Create some Python scripts. And yeah, regarding Python, you can call it from Python as well. So it's Python library. So you can, you can use it from common line, but you can also here is the common time usage. It can also import it. Load the model and transcribe your own data just passing as the reference to the to the file right and here are the examples how you can do that with with Python. Still, it has one problem, which probably being solved in some forks of the whisper, like whisper X and so on. The problem is it doesn't differentiate speakers. So you will not see the difference between speaker one and speaker two right. So what I did in homework I shared the file with you where there is a difference between speaker one and speaker two so here is no no any difference but the output formers may be different right so Let me call the help once again. Here output format you can see we have a lot of different output formers txt, tsrt, tsvgs on all. Let's try. Let's try to run this all but probably all is a default right so let me check. Okay, yeah, all is a default it means I already have all this information available so let's let's check it out. If you like Json's, you can run this. Let me format it. Very good, right.